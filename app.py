#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Application Streamlit - Analyse des opportunit√©s de Maillage Interne
===================================================================

Interface graphique pour analyser les opportunit√©s de maillage interne
bas√©es sur les donn√©es de la Google Search Console.

Auteur: JC avec Claude AI
Version: 18.4 - Version finale sans √©mojis avec documentation compl√®te
Date: 2025-11-04

## AM√âLIORATION: Changelog v18.4
- **INTERFACE √âPUR√âE:** Suppression de tous les √©mojis pour une interface professionnelle.
- **DOCUMENTATION INT√âGR√âE:** Ajout d'une box d'informations compl√®te avec m√©thodologie d√©taill√©e.
- **GUIDE SCREAMING FROG:** Instructions pr√©cises pour l'extraction HTML (mode "Stocker le HTML").
- **STABILIT√â:** Analyse combin√©e exacte + floue, barre de progression fiable, optimisations de performance.
"""

import streamlit as st
import pandas as pd
import zipfile
import io
import re
import urllib.parse
from collections import Counter
from bs4 import BeautifulSoup, NavigableString
from typing import Dict, List, Tuple, Optional, Any
from functools import lru_cache

# Gestion des d√©pendances optionnelles
try:
    import ahocorasick
    AHO_CORASICK_AVAILABLE = True
except ImportError:
    AHO_CORASICK_AVAILABLE = False
try:
    import openpyxl
    XLSX_EXPORT_AVAILABLE = True
except ImportError:
    XLSX_EXPORT_AVAILABLE = False
try:
    from fuzzywuzzy import fuzz
    FUZZY_AVAILABLE = True
except ImportError:
    FUZZY_AVAILABLE = False

st.set_page_config(
    page_title="Maillage Interne SEO", page_icon="üîó",
    layout="wide", initial_sidebar_state="expanded"
)

# --- CLASSE ANALYSEUR (Stable et compl√®te) ---
class InternalLinkingAnalyzer:
    FRENCH_STOPWORDS = {
        'a', '√†', 'au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', '√™tre', 'eu', 'il', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'm√™me', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ont', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', '√†', 'm', 'n', 's', 't', 'y', '√©t√©', '√©t√©e', '√©t√©es', '√©t√©s', '√©tant', 'suis', 'es', 'est', 'sommes', '√™tes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', '√©tais', '√©tait', '√©tions', '√©tiez', '√©taient', 'fus', 'fut', 'f√ªmes', 'f√ªtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'f√ªt', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'e√ªmes', 'e√ªtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'e√ªt', 'eussions', 'eussiez', 'eussent', 'ceci', 'cela', 'cel√†', 'cet', 'cette', 'ici', 'ils', 'les', 'leurs', 'quel', 'quels', 'quelle', 'quelles', 'sans', 'soi'
    }
    CLASSIC_PAGE_PATTERNS = [
        r'mentions[-_]?legales?', r'cgu', r'cgv', 'conditions', 'legal', 'a[-_]?propos', 'about', 'contact',
        r'nous[-_]?contacter', r'politique[-_]?confidentialite', 'privacy', 'cookie', r'plan[-_]?site',
        'sitemap', 'aide', 'help', 'faq', 'support', '404', 'erreur', r'recherche', 'search', 'connexion',
        'login', 'inscription', 'register', 'panier', 'cart', 'commande', 'checkout', r'mon[-_]?compte', 'account'
    ]
    def __init__(self, config: Dict):
        self.config = config
        self.excel_data = None
        
    def load_excel_data(self, uploaded_file) -> bool:
        try:
            df = pd.read_csv(uploaded_file, on_bad_lines='skip') if uploaded_file.name.endswith('.csv') else pd.read_excel(uploaded_file)
            df.columns = df.columns.str.lower().str.strip()
            rename_map = {'pages': 'page', 'requ√™te': 'query', 'clics': 'clicks', 'position moyenne': 'position'}
            df = df.rename(columns=rename_map)
            required_cols = ['page', 'query', 'clicks']
            if not all(col in df.columns for col in required_cols):
                st.error(f"Colonnes manquantes ! Requis: {', '.join(required_cols)}. Trouv√©: {list(df.columns)}"); return False
            df = df.dropna(subset=required_cols).copy()
            df['clicks'] = pd.to_numeric(df['clicks'], errors='coerce')
            if 'position' in df.columns: df['position'] = pd.to_numeric(df['position'], errors='coerce')
            df.dropna(subset=['clicks'], inplace=True)
            if 'query' in df.columns: df.dropna(subset=['query'], inplace=True)
            if self.config['min_clicks'] > 0: df = df[df['clicks'] >= self.config['min_clicks']]
            if 'position' in df.columns and self.config['max_position'] > 0: df = df[df['position'] <= self.config['max_position']]
            if 'query' in df.columns: df = df[df['query'].str.len() >= self.config['min_keyword_length']]
            if self.config['exclude_stopwords']: df = df[~df['query'].str.lower().isin(self.FRENCH_STOPWORDS)]
            df['priority'] = df['clicks'] * (1 / df['position'].clip(lower=0.1)) if 'position' in df.columns else df['clicks']
            self.excel_data = df
            return True
        except Exception as e:
            st.error(f"Erreur lors du chargement du fichier GSC: {e}")
            return False

    @staticmethod
    @lru_cache(maxsize=200_000)
    def _normalize_url_for_comparison(url: str) -> str:
        if not url: return ""
        try:
            url = url.lower()
            url = re.sub(r'(\?|&)(utm_.*|gclid|fbclid)=[^&]*', '', url)
            parsed = urllib.parse.urlparse(url)
            netloc = parsed.netloc.replace('www.', '')
            path = parsed.path.rstrip('/') or ''
            query = '?' + urllib.parse.urlencode(sorted(urllib.parse.parse_qsl(parsed.query))) if parsed.query else ''
            return f"{netloc}{path}{query}"
        except: return url.lower()

    def _is_classic_page(self, url: str) -> bool:
        if not self.config.get('exclude_classic_pages', True): return False
        for pattern in self.CLASSIC_PAGE_PATTERNS:
            if re.search(pattern, url.lower()): return True
        return False
        
    def _get_content_selectors(self) -> List[str]:
        selectors = self.config.get('content_selectors', ['p', 'li', 'span']).copy()
        if self.config.get('custom_class'):
            selectors.append(f".{self.config['custom_class']}")
        return selectors
        
    def detect_content_classes(self, zip_file_content: bytes) -> List[Tuple[str, int]]:
        if not self.config.get('auto_detect_classes', True): return []
        class_counter = Counter()
        with zipfile.ZipFile(io.BytesIO(zip_file_content), 'r') as zip_ref:
            html_files_info = [info for info in zip_ref.infolist() if info.filename.endswith('.html') and not info.is_dir()]
            for file_info in html_files_info[:500]:
                try:
                    soup = BeautifulSoup(zip_ref.read(file_info.filename), 'html.parser')
                    for element in soup.find_all(['div', 'section', 'article', 'main', 'p']):
                        if element.get('class') and len(element.get_text(strip=True)) > 100:
                            for cls in element.get('class'):
                                if not cls.startswith(('js-', 'css-')): class_counter[cls] += 1
                except Exception: continue
        return class_counter.most_common(10)

    @staticmethod
    def _find_anchor_location(element: BeautifulSoup, anchor_text: str) -> str:
        anchor_lower = anchor_text.lower()
        for img in element.find_all('img', alt=True):
            if anchor_lower in img['alt'].lower(): return "Attribut 'alt' (Image)"
        if element.has_attr('title') and anchor_lower in element['title'].lower(): return "Attribut 'title'"
        for child in element.find_all(title=True):
             if anchor_lower in child['title'].lower(): return "Attribut 'title'"
        return "Texte Principal"

    def analyze_opportunities(self, zip_file_content: bytes, selected_keywords: Optional[List[str]]) -> List[Dict]:
        if self.excel_data is None: return []
        opportunities = []
        selectors = self._get_content_selectors()
        keyword_index = {}
        working_data = self.excel_data.copy()
        if selected_keywords: working_data = working_data[working_data['query'].isin(selected_keywords)]
        for _, row in working_data.iterrows():
            query = row['query'].lower().strip()
            if query not in keyword_index or keyword_index[query]['priority'] < row['priority']:
                keyword_index[query] = {'page': row['page'], 'priority': row['priority'], 'clicks': row['clicks'], 'original_query': row['query']}
        if not keyword_index: return []
        
        A = None
        run_fuzzy = self.config.get('use_fuzzy_matching', False)
        if AHO_CORASICK_AVAILABLE:
            A = ahocorasick.Automaton()
            for keyword, data in keyword_index.items(): A.add_word(keyword, (keyword, data['original_query']))
            A.make_automaton()
        
        with zipfile.ZipFile(io.BytesIO(zip_file_content), 'r') as zip_ref:
            # L'indexation est rapide et mise en cache, on peut la refaire
            canonical_map = {}
            feedback_placeholder = st.empty()
            feedback_placeholder.text("Cr√©ation de l'index des pages HTML...")
            all_zip_files = [info for info in zip_ref.infolist() if info.filename.endswith('.html') and not info.is_dir()]
            map_progress = feedback_placeholder.progress(0)
            for i, file_info in enumerate(all_zip_files):
                if i % 100 == 0: map_progress.progress((i+1)/len(all_zip_files))
                try:
                    content = zip_ref.read(file_info.filename)
                    soup = BeautifulSoup(content, 'html.parser', from_encoding='utf-8')
                    canonical_link = soup.find('link', rel='canonical', href=True)
                    if canonical_link:
                        canonical_map[self._normalize_url_for_comparison(canonical_link['href'])] = file_info.filename
                except Exception: continue
            
            source_urls_to_scan = self.excel_data['page'].unique()
            max_pages = self.config.get('max_pages_to_analyze', len(source_urls_to_scan))
            urls_to_process = source_urls_to_scan[:max_pages]
            mapped_count = 0
            
            feedback_placeholder.text("Analyse des opportunit√©s en cours...")
            progress_bar = feedback_placeholder.progress(0)
            
            for i, source_url in enumerate(urls_to_process):
                progress_bar.progress((i + 1) / len(urls_to_process), text=f"Analyse... {source_url[:80]}")
                if self._is_classic_page(source_url): continue
                normalized_source_key = self._normalize_url_for_comparison(source_url)
                html_filename = canonical_map.get(normalized_source_key)
                if not html_filename: continue
                mapped_count += 1
                try:
                    html_content = zip_ref.read(html_filename).decode('utf-8', errors='ignore')
                    soup = BeautifulSoup(html_content, 'html.parser')
                    existing_links_normalized = {self._normalize_url_for_comparison(urllib.parse.urljoin(source_url, link.get('href'))) for link in soup.find_all('a', href=True) if link.get('href') and not link.get('href').startswith(('mailto:', 'tel:'))}
                    for element in soup.select(', '.join(selectors)):
                        text_content = element.get_text(" ", strip=True)
                        if len(text_content) < self.config.get('min_keyword_length', 3): continue
                        text_lower = text_content.lower()
                        
                        found_kws_in_element = set()
                        if A:
                            for _, (keyword, original_query) in A.iter(text_lower):
                                if keyword in found_kws_in_element: continue
                                found_kws_in_element.add(keyword)
                                opportunity = self._create_opportunity(original_query, keyword_index[keyword], source_url, existing_links_normalized, 'exact', element)
                                if opportunity: opportunities.append(opportunity)
                        
                        if run_fuzzy and FUZZY_AVAILABLE:
                            for keyword, data in keyword_index.items():
                                if keyword in found_kws_in_element: continue
                                similarity = fuzz.token_set_ratio(keyword, text_lower)
                                if similarity >= self.config.get('fuzzy_threshold', 85):
                                    found_kws_in_element.add(keyword)
                                    opportunity = self._create_opportunity(data['original_query'], data, source_url, existing_links_normalized, f'fuzzy ({similarity}%)', element)
                                    if opportunity: opportunities.append(opportunity)
                except Exception: continue
            
            feedback_placeholder.empty()
            if len(urls_to_process) > 0:
                st.info(f"Matching r√©ussi : {mapped_count} sur {len(urls_to_process)} URLs GSC analys√©es ont √©t√© trouv√©es dans le fichier ZIP ({mapped_count/len(urls_to_process):.1%}).")

        opportunities = [dict(t) for t in {tuple(d.items()) for d in opportunities}]
        opportunities.sort(key=lambda x: x['priority'], reverse=True)
        return opportunities

    def _create_opportunity(self, anchor_text, target_data, source_url, existing_links_normalized, match_type, element) -> Optional[Dict]:
        target_page_url = target_data['page']
        normalized_source = self._normalize_url_for_comparison(source_url)
        normalized_target = self._normalize_url_for_comparison(target_page_url)
        if normalized_source == normalized_target: return None
        link_exists = normalized_target in existing_links_normalized
        anchor_location = self._find_anchor_location(element, anchor_text)
        element_tag, classes = element.name, element.get('class', [])
        class_str = f".{'.'.join(classes)}" if classes else ""
        return {'source_url': source_url, 'target_url': target_page_url, 'anchor': anchor_text, 'priority': target_data['priority'], 'clicks': target_data['clicks'], 'match_type': match_type, 'element_source': f"<{element_tag}{class_str}>", 'existing_link': "[X] Lien pr√©sent" if link_exists else "[OK] Nouvelle opportunit√©", 'anchor_location': anchor_location}

# --- FONCTIONS DE LIAISON (pour le cache Streamlit) ---
@st.cache_data
def load_gsc_data_cached(uploaded_file, config):
    analyzer = InternalLinkingAnalyzer(config)
    if analyzer.load_excel_data(uploaded_file):
        return analyzer.excel_data
    return None

# --- INTERFACE STREAMLIT ---
def main():
    st.title("Analyseur de Maillage Interne SEO")
    st.markdown("**Strat√©gie 'Canonical First' : la solution la plus robuste pour une analyse fiable.**")
    
    # Box d'informations
    with st.expander("‚ÑπÔ∏è Comment utiliser cet outil ? (Cliquez pour d√©rouler)", expanded=False):
        st.markdown("""
        ### Principe de fonctionnement
        
        Cet outil analyse vos pages web pour identifier des **opportunit√©s de maillage interne** en croisant :
        - Les **mots-cl√©s performants** de votre Google Search Console (GSC)
        - Le **contenu textuel** de vos pages HTML
        
        ### M√©thodologie
        
        1. **Correspondance Canonical First** : L'outil utilise les balises canonical de vos pages HTML pour garantir un matching pr√©cis avec les URLs de la GSC
        2. **D√©tection intelligente** : Il cherche dans le contenu de vos pages les mots-cl√©s qui g√©n√®rent du trafic sur d'autres pages
        3. **Analyse des opportunit√©s** : Quand un mot-cl√© est trouv√©, l'outil v√©rifie si un lien existe d√©j√† vers la page cible
        
        ### √âtapes d'utilisation
        
        **√âtape 1 : Pr√©parez vos donn√©es GSC**
        - Exportez vos donn√©es depuis Google Search Console (Pages + Requ√™tes)
        - Format accept√© : Excel (.xlsx, .xls) ou CSV
        - Colonnes requises : `Page`, `Query` (ou Requ√™te), `Clicks` (ou Clics)
        - Colonne optionnelle : `Position` (ou Position Moyenne)
        
        **√âtape 2 : Cr√©ez votre archive HTML**
        
        **M√©thode recommand√©e : Screaming Frog SEO Spider**
        
        1. **Configuration du crawl** :
           - Ouvrez Screaming Frog et entrez l'URL de votre site
           - Allez dans `Configuration > Spider > Rendu`
           - **Important** : S√©lectionnez **"Stocker le HTML"** (et non "Rendu JavaScript")
           
        2. **Pourquoi "Stocker le HTML" ?**
           - Si votre site est bien structur√© avec du contenu HTML natif (pas uniquement g√©n√©r√© en JS)
           - Cette m√©thode est **beaucoup plus rapide** et consomme moins de ressources
           - Le HTML brut contient d√©j√† le contenu textuel n√©cessaire pour l'analyse
           - Les balises canonical sont pr√©sentes dans le HTML source
           
        3. **Quand utiliser le rendu JavaScript ?**
           - Uniquement si votre site est une SPA (Single Page Application) type React/Vue/Angular
           - Si le contenu textuel n'appara√Æt qu'apr√®s ex√©cution du JavaScript
           - Note : Le rendu JS ralentit consid√©rablement le crawl (jusqu'√† 10x plus lent)
           
        4. **Lancez le crawl** :
           - Cliquez sur "D√©marrer" et attendez la fin du crawl
           - V√©rifiez que toutes vos pages importantes ont √©t√© crawl√©es
           
        5. **Export du HTML** :
           - Allez dans `Export > HTML/Bulk Export > HTML`
           - Screaming Frog cr√©e automatiquement un dossier avec tous les fichiers HTML
           
        6. **Cr√©ez le fichier ZIP** :
           - Compressez l'int√©gralit√© du dossier HTML export√© dans un fichier ZIP
           - Important : Les pages doivent contenir les balises `<link rel="canonical">`
        
        **Alternative : Autres crawlers**
        - **Sitebulb** : Export HTML disponible dans les rapports
        - **OnCrawl** : Export HTML via l'API ou l'interface
        - **Crawl manuel** : wget ou curl avec compression ZIP des r√©sultats
        
        **√âtape 3 : Configurez l'analyse (Sidebar)**
        - **Filtres de donn√©es** : D√©finissez des seuils (clics minimum, position max, etc.)
        - **Exclusions** : Filtrez les stop-words et pages classiques (contact, CGU, etc.)
        - **Analyse floue** : Activez pour d√©tecter aussi les variations de mots-cl√©s (pluriels, etc.)
        - **Ciblage du contenu** : S√©lectionnez les balises HTML √† analyser (p, li, span, etc.)
        
        **√âtape 4 : Uploadez vos fichiers**
        - Uploadez d'abord votre fichier GSC (colonne de gauche)
        - Puis uploadez votre fichier ZIP HTML (colonne de droite)
        
        **√âtape 5 : Lancez l'analyse**
        - Cliquez sur "Lancer l'Analyse Compl√®te"
        - L'outil va traiter vos donn√©es (cela peut prendre quelques minutes selon la taille)
        
        **√âtape 6 : Exploitez les r√©sultats**
        - Consultez le tableau des opportunit√©s d√©tect√©es
        - Exportez les r√©sultats en CSV ou Excel
        - Priorisez vos actions selon la colonne "Priorit√©" (bas√©e sur clics √ó position)
        
        ### Interpr√©tation des r√©sultats
        
        - **[OK] Nouvelle opportunit√©** : Aucun lien n'existe, c'est une vraie opportunit√© de maillage
        - **[X] Lien pr√©sent** : Un lien existe d√©j√†, pas d'action n√©cessaire
        - **Type de Match** : 
          - `exact` : correspondance exacte du mot-cl√©
          - `fuzzy (X%)` : correspondance approximative (variante d√©tect√©e)
        - **Source Ancre** : Indique o√π se trouve le texte d'ancre potentiel (texte principal, alt d'image, etc.)
        - **Priorit√©** : Score calcul√© selon les clics et la position du mot-cl√© dans la GSC
        
        ### Conseils d'optimisation
        
        - Commencez par analyser les **nouvelles opportunit√©s** avec la plus haute priorit√©
        - V√©rifiez la pertinence contextuelle avant d'ajouter un lien
        - Utilisez le texte d'ancre sugg√©r√© ou adaptez-le selon le contexte
        - Privil√©giez les liens dans le contenu principal (balises `<p>`) plut√¥t que dans les sidebars
        
        ### Questions fr√©quentes
        
        **Pourquoi certaines URLs GSC ne sont pas trouv√©es ?**
        - Les URLs doivent correspondre exactement via leur balise canonical
        - V√©rifiez que votre crawl HTML est complet
        - Les pages dynamiques ou prot√©g√©es peuvent ne pas √™tre crawl√©es
        
        **L'analyse est lente, comment l'acc√©l√©rer ?**
        - R√©duisez la "Limite de pages √† analyser" dans la configuration
        - Installez `pyahocorasick` pour de meilleures performances
        - D√©sactivez l'analyse floue si vous n'en avez pas besoin
        
        **Que faire si je n'ai pas de balises canonical ?**
        - Cet outil n√©cessite des balises canonical pour fonctionner de mani√®re fiable
        - Ajoutez-les √† vos pages avant de lancer l'analyse
        """)
    
    if 'config' not in st.session_state:
        st.session_state.config = {
            'min_clicks': 0, 'min_keyword_length': 3, 'exclude_stopwords': True, 'exclude_classic_pages': True,
            'content_selectors': ['p', 'li', 'span'], 'custom_class': '', 'max_position': 50,
            'manual_keyword_selection': False, 'auto_detect_classes': True, 'max_pages_to_analyze': 10000,
            'use_fuzzy_matching': False, 'fuzzy_threshold': 85
        }
    if 'gsc_data' not in st.session_state: st.session_state.gsc_data = None
    if 'zip_content' not in st.session_state: st.session_state.zip_content = None
    if 'results' not in st.session_state: st.session_state.results = None
    if 'detected_classes_list' not in st.session_state: st.session_state.detected_classes_list = []
    
    st.sidebar.header("Configuration")
    cfg = st.session_state.config
    st.sidebar.subheader("Filtres de Donn√©es")
    cfg['min_clicks'] = st.sidebar.number_input("Minimum de clics", 0, 1000, cfg.get('min_clicks', 0), help="Ignorer les mots-cl√©s qui ont g√©n√©r√© moins de clics que ce seuil.")
    cfg['min_keyword_length'] = st.sidebar.number_input("Longueur min. mots-cl√©s", 1, 20, cfg.get('min_keyword_length', 3), help="Ignorer les mots-cl√©s plus courts que ce nombre de caract√®res.")
    cfg['max_position'] = st.sidebar.number_input("Position max. SERPs", 0, 100, cfg.get('max_position', 50), help="Ignorer les mots-cl√©s dont la position moyenne est au-del√† de ce seuil (0 = pas de limite).")
    st.sidebar.subheader("Optimisation")
    cfg['max_pages_to_analyze'] = st.sidebar.number_input("Limite de pages √† analyser (GSC)", 100, 500000, cfg.get('max_pages_to_analyze', 10000), help="Limite le nombre d'URLs GSC uniques √† analyser pour acc√©l√©rer le traitement sur de tr√®s gros sites.")
    st.sidebar.subheader("Exclusions")
    cfg['exclude_stopwords'] = st.sidebar.checkbox("Exclure les stop words", cfg.get('exclude_stopwords', True), help="Exclut les mots vides courants (le, la, de, etc.) de l'analyse.")
    cfg['exclude_classic_pages'] = st.sidebar.checkbox("Exclure pages classiques", cfg.get('exclude_classic_pages', True), help="Exclut les pages comme 'contact', 'mentions l√©gales', 'CGU', etc.")
    st.sidebar.subheader("Analyse Floue")
    if FUZZY_AVAILABLE:
        cfg['use_fuzzy_matching'] = st.sidebar.checkbox("Activer l'analyse floue", cfg.get('use_fuzzy_matching', False), help="En plus de la recherche exacte, cherche des variations de mots-cl√©s (pluriels, synonymes...). Rend l'analyse plus lente.")
        if cfg['use_fuzzy_matching']:
            cfg['fuzzy_threshold'] = st.sidebar.slider("Seuil de similarit√© (%)", 70, 100, cfg.get('fuzzy_threshold', 85), help="Seuil √† partir duquel une variation est consid√©r√©e comme une opportunit√©.")
    else:
        st.sidebar.warning("Pour l'analyse floue, installez `fuzzywuzzy` et `python-levenshtein`.", icon="‚ö†Ô∏è")
        cfg['use_fuzzy_matching'] = False
    st.sidebar.subheader("Ciblage du Contenu")
    cfg['manual_keyword_selection'] = st.sidebar.checkbox("S√©lection manuelle des mots-cl√©s", cfg.get('manual_keyword_selection', False), help="Permet de choisir manuellement les mots-cl√©s √† analyser au lieu de tous les prendre.")
    cfg['auto_detect_classes'] = st.sidebar.checkbox("D√©tection auto des classes CSS", cfg.get('auto_detect_classes', True), help="Analyse le HTML pour trouver les classes CSS contenant le plus de texte.")
    cfg['content_selectors'] = st.sidebar.multiselect("S√©lecteurs de contenu", ['p', 'li', 'span', 'div', 'h1', 'h2', 'h3'], cfg.get('content_selectors', ['p', 'li', 'span']), help="Balises HTML dans lesquelles chercher les opportunit√©s.")
    if st.session_state.detected_classes_list:
        selected_class = st.sidebar.selectbox("Utiliser une classe CSS d√©tect√©e ?", options=[''] + st.session_state.detected_classes_list, help="Cible l'analyse sur une classe CSS sp√©cifique trouv√©e lors de la d√©tection automatique.")
        cfg['custom_class'] = selected_class
    else:
        cfg['custom_class'] = st.sidebar.text_input("Ajouter une classe CSS manuellement", cfg.get('custom_class', ''), help="Entrez une classe CSS (sans le '.') pour cibler une zone de contenu sp√©cifique.")

    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Donn√©es Google Search Console")
        excel_file = st.file_uploader("Uploadez votre fichier Excel/CSV", type=['xlsx', 'xls', 'csv'])
        if excel_file:
            st.session_state.gsc_data = load_gsc_data_cached(excel_file, cfg)
            if st.session_state.gsc_data is not None: st.success(f"Donn√©es GSC charg√©es: {len(st.session_state.gsc_data)} lignes.")
    with col2:
        st.subheader("Fichiers HTML")
        if st.session_state.gsc_data is not None:
            zip_file = st.file_uploader("Uploadez le fichier ZIP HTML", type=['zip'])
            if zip_file:
                st.session_state.zip_content = zip_file.getvalue()
                st.success(f"Fichier ZIP charg√© ({len(st.session_state.zip_content)/1e6:.2f} MB).")
                if cfg['auto_detect_classes'] and not st.session_state.detected_classes_list:
                    with st.spinner("D√©tection des classes CSS..."):
                        analyzer = InternalLinkingAnalyzer(cfg)
                        st.session_state.detected_classes_list = [cls for cls, _ in analyzer.detect_content_classes(st.session_state.zip_content)]
                        if st.session_state.detected_classes_list: st.rerun()
        else: st.info("Veuillez d'abord charger les donn√©es Excel.")

    selected_keywords = None
    if st.session_state.gsc_data is not None and cfg['manual_keyword_selection']:
        st.subheader("S√©lection des Mots-cl√©s √† Analyser")
        available_keywords = sorted(st.session_state.gsc_data['query'].unique().tolist())
        selected_keywords = st.multiselect("S√©lectionnez les mots-cl√©s:", options=available_keywords)
    
    if st.session_state.gsc_data is not None and st.session_state.zip_content is not None:
        can_analyze = not cfg['manual_keyword_selection'] or (cfg['manual_keyword_selection'] and selected_keywords is not None)
        if can_analyze:
            if st.button("Lancer l'Analyse Compl√®te", type="primary", use_container_width=True):
                analyzer = InternalLinkingAnalyzer(cfg)
                analyzer.excel_data = st.session_state.gsc_data
                st.session_state.results = analyzer.analyze_opportunities(st.session_state.zip_content, selected_keywords)
        elif cfg['manual_keyword_selection']:
            st.warning("Veuillez s√©lectionner au moins un mot-cl√© pour lancer l'analyse.")

    if st.session_state.results is not None:
        if st.session_state.results:
            df_display = pd.DataFrame(st.session_state.results).rename(columns={'source_url': 'URL Source', 'target_url': 'Page √† Mailler', 'anchor': 'Ancre de Lien', 'element_source': '√âl√©ment Source', 'existing_link': 'Lien Existant', 'priority': 'Priorit√©', 'match_type': 'Type de Match', 'anchor_location': 'Source Ancre'})
            st.header("R√©sultats de l'Analyse")
            st.dataframe(df_display[['URL Source', 'Ancre de Lien', 'Source Ancre', 'Page √† Mailler', '√âl√©ment Source', 'Type de Match', 'Lien Existant', 'Priorit√©']], use_container_width=True, column_config={"URL Source": st.column_config.LinkColumn(), "Page √† Mailler": st.column_config.LinkColumn()})
            st.subheader("Export des R√©sultats")
            col_export1, col_export2 = st.columns(2)
            with col_export1:
                st.download_button("T√©l√©charger CSV", df_display.to_csv(index=False, encoding='utf-8-sig'), "opportunites_maillage.csv", "text/csv", use_container_width=True)
            with col_export2:
                if XLSX_EXPORT_AVAILABLE:
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer: df_display.to_excel(writer, index=False, sheet_name='Opportunit√©s')
                    st.download_button("T√©l√©charger Excel (.xlsx)", output.getvalue(), "opportunites_maillage.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", use_container_width=True)
                else: st.warning("Pour l'export Excel, installez `openpyxl`", icon="‚ö†Ô∏è")
            st.divider()
            st.header("Tableau de Bord de l'Analyse")
            total_ops, new_ops = len(df_display), len(df_display[df_display['Lien Existant'] == '[OK] Nouvelle opportunit√©'])
            col_metric1, col_metric2, col_metric3 = st.columns(3)
            col_metric1.metric("Opportunit√©s Totales", total_ops)
            if total_ops > 0:
                col_metric2.metric("Nouvelles Opportunit√©s [OK]", new_ops, f"{new_ops/total_ops:.1%}")
                col_metric3.metric("Liens D√©j√† Pr√©sents [X]", total_ops - new_ops, f"{(total_ops - new_ops)/total_ops:.1%}")
            col_graph1, col_graph2 = st.columns(2)
            with col_graph1:
                st.write("**Top 10 Pages Sources d'Opportunit√©s**"); st.bar_chart(df_display['URL Source'].value_counts().head(10))
                st.write("**Distribution par Type de Match**"); st.bar_chart(df_display['Type de Match'].value_counts())
            with col_graph2:
                st.write("**Top 10 Pages Cibles (√† mailler)**"); st.bar_chart(df_display['Page √† Mailler'].value_counts().head(10))
                st.write("**Distribution par Source de l'Ancre**"); st.bar_chart(df_display['Source Ancre'].value_counts())
        else:
            st.warning("Aucune opportunit√© trouv√©e avec la configuration actuelle.")
            
    st.sidebar.divider()
    if st.sidebar.button("Recommencer l'analyse"):
        for key in list(st.session_state.keys()): del st.session_state[key]
        st.cache_data.clear()
        st.rerun()

if __name__ == "__main__":
    if not AHO_CORASICK_AVAILABLE: st.warning("**Performance limit√©e :** `pyahocorasick` non install√© (`pip install pyahocorasack`)", icon="‚ö†Ô∏è")
    if not XLSX_EXPORT_AVAILABLE: st.sidebar.warning("Pour l'export Excel (.xlsx), installez `openpyxl`", icon="‚ö†Ô∏è")
    if not FUZZY_AVAILABLE: st.sidebar.warning("Pour l'analyse floue, installez `fuzzywuzzy` et `python-levenshtein`", icon="‚ö†Ô∏è")
    main()
