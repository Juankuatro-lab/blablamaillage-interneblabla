#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Application Streamlit - Analyse des Opportunit√©s de Maillage Interne
===================================================================

Interface graphique pour analyser les opportunit√©s de maillage interne
bas√©es sur les donn√©es de la Google Search Console.

Auteur: Assistant Claude
Version: 18.3 - The Simple & Robust Final Version
Date: 2025-07-14

## AM√âLIORATION: Changelog v18.3
- **LOGIQUE SIMPLIFI√âE:** Retour √† une seule analyse pilot√©e par la checkbox "Analyse Floue".
  - Si d√©coch√©e : analyse exacte rapide.
  - Si coch√©e : analyse exacte + floue combin√©es.
- **FEEDBACK FIABLE:** Une seule barre de progression pour un suivi clair et constant.
- **STABILIT√â FINALE:** Cette version est la plus stable et directe, combinant toutes les
  fonctionnalit√©s (Source de l'Ancre, Fuzzy, etc.) et les optimisations de performance.
"""

import streamlit as st
import pandas as pd
import zipfile
import io
import re
import urllib.parse
from collections import Counter
from bs4 import BeautifulSoup, NavigableString
from typing import Dict, List, Tuple, Optional, Any
from functools import lru_cache

# Gestion des d√©pendances optionnelles
try:
    import ahocorasick
    AHO_CORASICK_AVAILABLE = True
except ImportError:
    AHO_CORASICK_AVAILABLE = False
try:
    import openpyxl
    XLSX_EXPORT_AVAILABLE = True
except ImportError:
    XLSX_EXPORT_AVAILABLE = False
try:
    from fuzzywuzzy import fuzz
    FUZZY_AVAILABLE = True
except ImportError:
    FUZZY_AVAILABLE = False

st.set_page_config(
    page_title="üîó Maillage Interne SEO", page_icon="üîó",
    layout="wide", initial_sidebar_state="expanded"
)

# --- CLASSE ANALYSEUR (Stable et compl√®te) ---
class InternalLinkingAnalyzer:
    FRENCH_STOPWORDS = {
        'a', '√†', 'au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', '√™tre', 'eu', 'il', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'm√™me', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ont', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', '√†', 'm', 'n', 's', 't', 'y', '√©t√©', '√©t√©e', '√©t√©es', '√©t√©s', '√©tant', 'suis', 'es', 'est', 'sommes', '√™tes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', '√©tais', '√©tait', '√©tions', '√©tiez', '√©taient', 'fus', 'fut', 'f√ªmes', 'f√ªtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'f√ªt', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'e√ªmes', 'e√ªtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'e√ªt', 'eussions', 'eussiez', 'eussent', 'ceci', 'cela', 'cel√†', 'cet', 'cette', 'ici', 'ils', 'les', 'leurs', 'quel', 'quels', 'quelle', 'quelles', 'sans', 'soi'
    }
    CLASSIC_PAGE_PATTERNS = [
        r'mentions[-_]?legales?', r'cgu', r'cgv', 'conditions', 'legal', 'a[-_]?propos', 'about', 'contact',
        r'nous[-_]?contacter', r'politique[-_]?confidentialite', 'privacy', 'cookie', r'plan[-_]?site',
        'sitemap', 'aide', 'help', 'faq', 'support', '404', 'erreur', r'recherche', 'search', 'connexion',
        'login', 'inscription', 'register', 'panier', 'cart', 'commande', 'checkout', r'mon[-_]?compte', 'account'
    ]
    def __init__(self, config: Dict):
        self.config = config
        self.excel_data = None
        
    def load_excel_data(self, uploaded_file) -> bool:
        try:
            df = pd.read_csv(uploaded_file, on_bad_lines='skip') if uploaded_file.name.endswith('.csv') else pd.read_excel(uploaded_file)
            df.columns = df.columns.str.lower().str.strip()
            rename_map = {'pages': 'page', 'requ√™te': 'query', 'clics': 'clicks', 'position moyenne': 'position'}
            df = df.rename(columns=rename_map)
            required_cols = ['page', 'query', 'clicks']
            if not all(col in df.columns for col in required_cols):
                st.error(f"Colonnes manquantes ! Requis: {', '.join(required_cols)}. Trouv√©: {list(df.columns)}"); return False
            df = df.dropna(subset=required_cols).copy()
            df['clicks'] = pd.to_numeric(df['clicks'], errors='coerce')
            if 'position' in df.columns: df['position'] = pd.to_numeric(df['position'], errors='coerce')
            df.dropna(subset=['clicks'], inplace=True)
            if 'query' in df.columns: df.dropna(subset=['query'], inplace=True)
            if self.config['min_clicks'] > 0: df = df[df['clicks'] >= self.config['min_clicks']]
            if 'position' in df.columns and self.config['max_position'] > 0: df = df[df['position'] <= self.config['max_position']]
            if 'query' in df.columns: df = df[df['query'].str.len() >= self.config['min_keyword_length']]
            if self.config['exclude_stopwords']: df = df[~df['query'].str.lower().isin(self.FRENCH_STOPWORDS)]
            df['priority'] = df['clicks'] * (1 / df['position'].clip(lower=0.1)) if 'position' in df.columns else df['clicks']
            self.excel_data = df
            return True
        except Exception as e:
            st.error(f"Erreur lors du chargement du fichier GSC: {e}")
            return False

    @staticmethod
    @lru_cache(maxsize=200_000)
    def _normalize_url_for_comparison(url: str) -> str:
        if not url: return ""
        try:
            url = url.lower()
            url = re.sub(r'(\?|&)(utm_.*|gclid|fbclid)=[^&]*', '', url)
            parsed = urllib.parse.urlparse(url)
            netloc = parsed.netloc.replace('www.', '')
            path = parsed.path.rstrip('/') or ''
            query = '?' + urllib.parse.urlencode(sorted(urllib.parse.parse_qsl(parsed.query))) if parsed.query else ''
            return f"{netloc}{path}{query}"
        except: return url.lower()

    def _is_classic_page(self, url: str) -> bool:
        if not self.config.get('exclude_classic_pages', True): return False
        for pattern in self.CLASSIC_PAGE_PATTERNS:
            if re.search(pattern, url.lower()): return True
        return False
        
    def _get_content_selectors(self) -> List[str]:
        selectors = self.config.get('content_selectors', ['p', 'li', 'span']).copy()
        if self.config.get('custom_class'):
            selectors.append(f".{self.config['custom_class']}")
        return selectors
        
    def detect_content_classes(self, zip_file_content: bytes) -> List[Tuple[str, int]]:
        if not self.config.get('auto_detect_classes', True): return []
        class_counter = Counter()
        with zipfile.ZipFile(io.BytesIO(zip_file_content), 'r') as zip_ref:
            html_files_info = [info for info in zip_ref.infolist() if info.filename.endswith('.html') and not info.is_dir()]
            for file_info in html_files_info[:500]:
                try:
                    soup = BeautifulSoup(zip_ref.read(file_info.filename), 'html.parser')
                    for element in soup.find_all(['div', 'section', 'article', 'main', 'p']):
                        if element.get('class') and len(element.get_text(strip=True)) > 100:
                            for cls in element.get('class'):
                                if not cls.startswith(('js-', 'css-')): class_counter[cls] += 1
                except Exception: continue
        return class_counter.most_common(10)

    @staticmethod
    def _find_anchor_location(element: BeautifulSoup, anchor_text: str) -> str:
        anchor_lower = anchor_text.lower()
        for img in element.find_all('img', alt=True):
            if anchor_lower in img['alt'].lower(): return "Attribut 'alt' (Image)"
        if element.has_attr('title') and anchor_lower in element['title'].lower(): return "Attribut 'title'"
        for child in element.find_all(title=True):
             if anchor_lower in child['title'].lower(): return "Attribut 'title'"
        return "Texte Principal"

    def analyze_opportunities(self, zip_file_content: bytes, selected_keywords: Optional[List[str]]) -> List[Dict]:
        if self.excel_data is None: return []
        opportunities = []
        selectors = self._get_content_selectors()
        keyword_index = {}
        working_data = self.excel_data.copy()
        if selected_keywords: working_data = working_data[working_data['query'].isin(selected_keywords)]
        for _, row in working_data.iterrows():
            query = row['query'].lower().strip()
            if query not in keyword_index or keyword_index[query]['priority'] < row['priority']:
                keyword_index[query] = {'page': row['page'], 'priority': row['priority'], 'clicks': row['clicks'], 'original_query': row['query']}
        if not keyword_index: return []
        
        A = None
        run_fuzzy = self.config.get('use_fuzzy_matching', False)
        if AHO_CORASICK_AVAILABLE:
            A = ahocorasick.Automaton()
            for keyword, data in keyword_index.items(): A.add_word(keyword, (keyword, data['original_query']))
            A.make_automaton()
        
        with zipfile.ZipFile(io.BytesIO(zip_file_content), 'r') as zip_ref:
            # L'indexation est rapide et mise en cache, on peut la refaire
            canonical_map = {}
            feedback_placeholder = st.empty()
            feedback_placeholder.text("Cr√©ation de l'index des pages HTML...")
            all_zip_files = [info for info in zip_ref.infolist() if info.filename.endswith('.html') and not info.is_dir()]
            map_progress = feedback_placeholder.progress(0)
            for i, file_info in enumerate(all_zip_files):
                if i % 100 == 0: map_progress.progress((i+1)/len(all_zip_files))
                try:
                    content = zip_ref.read(file_info.filename)
                    soup = BeautifulSoup(content, 'html.parser', from_encoding='utf-8')
                    canonical_link = soup.find('link', rel='canonical', href=True)
                    if canonical_link:
                        canonical_map[self._normalize_url_for_comparison(canonical_link['href'])] = file_info.filename
                except Exception: continue
            
            source_urls_to_scan = self.excel_data['page'].unique()
            max_pages = self.config.get('max_pages_to_analyze', len(source_urls_to_scan))
            urls_to_process = source_urls_to_scan[:max_pages]
            mapped_count = 0
            
            feedback_placeholder.text("Analyse des opportunit√©s en cours...")
            progress_bar = feedback_placeholder.progress(0)
            
            for i, source_url in enumerate(urls_to_process):
                progress_bar.progress((i + 1) / len(urls_to_process), text=f"Analyse... {source_url[:80]}")
                if self._is_classic_page(source_url): continue
                normalized_source_key = self._normalize_url_for_comparison(source_url)
                html_filename = canonical_map.get(normalized_source_key)
                if not html_filename: continue
                mapped_count += 1
                try:
                    html_content = zip_ref.read(html_filename).decode('utf-8', errors='ignore')
                    soup = BeautifulSoup(html_content, 'html.parser')
                    existing_links_normalized = {self._normalize_url_for_comparison(urllib.parse.urljoin(source_url, link.get('href'))) for link in soup.find_all('a', href=True) if link.get('href') and not link.get('href').startswith(('mailto:', 'tel:'))}
                    for element in soup.select(', '.join(selectors)):
                        text_content = element.get_text(" ", strip=True)
                        if len(text_content) < self.config.get('min_keyword_length', 3): continue
                        text_lower = text_content.lower()
                        
                        found_kws_in_element = set()
                        if A:
                            for _, (keyword, original_query) in A.iter(text_lower):
                                if keyword in found_kws_in_element: continue
                                found_kws_in_element.add(keyword)
                                opportunity = self._create_opportunity(original_query, keyword_index[keyword], source_url, existing_links_normalized, 'exact', element)
                                if opportunity: opportunities.append(opportunity)
                        
                        if run_fuzzy and FUZZY_AVAILABLE:
                            for keyword, data in keyword_index.items():
                                if keyword in found_kws_in_element: continue
                                similarity = fuzz.token_set_ratio(keyword, text_lower)
                                if similarity >= self.config.get('fuzzy_threshold', 85):
                                    found_kws_in_element.add(keyword)
                                    opportunity = self._create_opportunity(data['original_query'], data, source_url, existing_links_normalized, f'fuzzy ({similarity}%)', element)
                                    if opportunity: opportunities.append(opportunity)
                except Exception: continue
            
            feedback_placeholder.empty()
            if len(urls_to_process) > 0:
                st.info(f"üîó **Matching r√©ussi :** {mapped_count} sur {len(urls_to_process)} URLs GSC analys√©es ont √©t√© trouv√©es dans le fichier ZIP ({mapped_count/len(urls_to_process):.1%}).")

        opportunities = [dict(t) for t in {tuple(d.items()) for d in opportunities}]
        opportunities.sort(key=lambda x: x['priority'], reverse=True)
        return opportunities

    def _create_opportunity(self, anchor_text, target_data, source_url, existing_links_normalized, match_type, element) -> Optional[Dict]:
        target_page_url = target_data['page']
        normalized_source = self._normalize_url_for_comparison(source_url)
        normalized_target = self._normalize_url_for_comparison(target_page_url)
        if normalized_source == normalized_target: return None
        link_exists = normalized_target in existing_links_normalized
        anchor_location = self._find_anchor_location(element, anchor_text)
        element_tag, classes = element.name, element.get('class', [])
        class_str = f".{'.'.join(classes)}" if classes else ""
        return {'source_url': source_url, 'target_url': target_page_url, 'anchor': anchor_text, 'priority': target_data['priority'], 'clicks': target_data['clicks'], 'match_type': match_type, 'element_source': f"<{element_tag}{class_str}>", 'existing_link': "‚ùå Lien pr√©sent" if link_exists else "‚úÖ Nouvelle opportunit√©", 'anchor_location': anchor_location}

# --- FONCTIONS DE LIAISON (pour le cache Streamlit) ---
@st.cache_data
def load_gsc_data_cached(uploaded_file, config):
    analyzer = InternalLinkingAnalyzer(config)
    if analyzer.load_excel_data(uploaded_file):
        return analyzer.excel_data
    return None

# --- INTERFACE STREAMLIT ---
def main():
    st.title("üîó Analyseur de Maillage Interne SEO")
    st.markdown("**Strat√©gie 'Canonical First' : la solution la plus robuste pour une analyse fiable.**")
    
    if 'config' not in st.session_state:
        st.session_state.config = {
            'min_clicks': 0, 'min_keyword_length': 3, 'exclude_stopwords': True, 'exclude_classic_pages': True,
            'content_selectors': ['p', 'li', 'span'], 'custom_class': '', 'max_position': 50,
            'manual_keyword_selection': False, 'auto_detect_classes': True, 'max_pages_to_analyze': 10000,
            'use_fuzzy_matching': False, 'fuzzy_threshold': 85
        }
    if 'gsc_data' not in st.session_state: st.session_state.gsc_data = None
    if 'zip_content' not in st.session_state: st.session_state.zip_content = None
    if 'results' not in st.session_state: st.session_state.results = None
    if 'detected_classes_list' not in st.session_state: st.session_state.detected_classes_list = []
    
    st.sidebar.header("‚öôÔ∏è Configuration")
    cfg = st.session_state.config
    st.sidebar.subheader("üéØ Filtres de Donn√©es")
    cfg['min_clicks'] = st.sidebar.number_input("Minimum de clics", 0, 1000, cfg.get('min_clicks', 0), help="Ignorer les mots-cl√©s qui ont g√©n√©r√© moins de clics que ce seuil.")
    cfg['min_keyword_length'] = st.sidebar.number_input("Longueur min. mots-cl√©s", 1, 20, cfg.get('min_keyword_length', 3), help="Ignorer les mots-cl√©s plus courts que ce nombre de caract√®res.")
    cfg['max_position'] = st.sidebar.number_input("Position max. SERPs", 0, 100, cfg.get('max_position', 50), help="Ignorer les mots-cl√©s dont la position moyenne est au-del√† de ce seuil (0 = pas de limite).")
    st.sidebar.subheader("‚ö°Ô∏è Optimisation")
    cfg['max_pages_to_analyze'] = st.sidebar.number_input("Limite de pages √† analyser (GSC)", 100, 500000, cfg.get('max_pages_to_analyze', 10000), help="Limite le nombre d'URLs GSC uniques √† analyser pour acc√©l√©rer le traitement sur de tr√®s gros sites.")
    st.sidebar.subheader("üö´ Exclusions")
    cfg['exclude_stopwords'] = st.sidebar.checkbox("Exclure les stop words", cfg.get('exclude_stopwords', True), help="Exclut les mots vides courants (le, la, de, etc.) de l'analyse.")
    cfg['exclude_classic_pages'] = st.sidebar.checkbox("Exclure pages classiques", cfg.get('exclude_classic_pages', True), help="Exclut les pages comme 'contact', 'mentions l√©gales', 'CGU', etc.")
    st.sidebar.subheader("üîç Analyse Floue")
    if FUZZY_AVAILABLE:
        cfg['use_fuzzy_matching'] = st.sidebar.checkbox("Activer l'analyse floue", cfg.get('use_fuzzy_matching', False), help="En plus de la recherche exacte, cherche des variations de mots-cl√©s (pluriels, synonymes...). Rend l'analyse plus lente.")
        if cfg['use_fuzzy_matching']:
            cfg['fuzzy_threshold'] = st.sidebar.slider("Seuil de similarit√© (%)", 70, 100, cfg.get('fuzzy_threshold', 85), help="Seuil √† partir duquel une variation est consid√©r√©e comme une opportunit√©.")
    else:
        st.sidebar.warning("Pour l'analyse floue, installez `fuzzywuzzy` et `python-levenshtein`.", icon="‚ö†Ô∏è")
        cfg['use_fuzzy_matching'] = False
    st.sidebar.subheader("üéØ Ciblage du Contenu")
    cfg['manual_keyword_selection'] = st.sidebar.checkbox("üéØ S√©lection manuelle des mots-cl√©s", cfg.get('manual_keyword_selection', False), help="Permet de choisir manuellement les mots-cl√©s √† analyser au lieu de tous les prendre.")
    cfg['auto_detect_classes'] = st.sidebar.checkbox("ü§ñ D√©tection auto des classes CSS", cfg.get('auto_detect_classes', True), help="Analyse le HTML pour trouver les classes CSS contenant le plus de texte.")
    cfg['content_selectors'] = st.sidebar.multiselect("S√©lecteurs de contenu", ['p', 'li', 'span', 'div', 'h1', 'h2', 'h3'], cfg.get('content_selectors', ['p', 'li', 'span']), help="Balises HTML dans lesquelles chercher les opportunit√©s.")
    if st.session_state.detected_classes_list:
        selected_class = st.sidebar.selectbox("Utiliser une classe CSS d√©tect√©e ?", options=[''] + st.session_state.detected_classes_list, help="Cible l'analyse sur une classe CSS sp√©cifique trouv√©e lors de la d√©tection automatique.")
        cfg['custom_class'] = selected_class
    else:
        cfg['custom_class'] = st.sidebar.text_input("Ajouter une classe CSS manuellement", cfg.get('custom_class', ''), help="Entrez une classe CSS (sans le '.') pour cibler une zone de contenu sp√©cifique.")

    col1, col2 = st.columns(2)
    with col1:
        st.subheader("üìä Donn√©es Google Search Console")
        excel_file = st.file_uploader("Uploadez votre fichier Excel/CSV", type=['xlsx', 'xls', 'csv'])
        if excel_file:
            st.session_state.gsc_data = load_gsc_data_cached(excel_file, cfg)
            if st.session_state.gsc_data is not None: st.success(f"Donn√©es GSC charg√©es: {len(st.session_state.gsc_data)} lignes.")
    with col2:
        st.subheader("üìÅ Fichiers HTML")
        if st.session_state.gsc_data is not None:
            zip_file = st.file_uploader("Uploadez le fichier ZIP HTML", type=['zip'])
            if zip_file:
                st.session_state.zip_content = zip_file.getvalue()
                st.success(f"Fichier ZIP charg√© ({len(st.session_state.zip_content)/1e6:.2f} MB).")
                if cfg['auto_detect_classes'] and not st.session_state.detected_classes_list:
                    with st.spinner("D√©tection des classes CSS..."):
                        analyzer = InternalLinkingAnalyzer(cfg)
                        st.session_state.detected_classes_list = [cls for cls, _ in analyzer.detect_content_classes(st.session_state.zip_content)]
                        if st.session_state.detected_classes_list: st.rerun()
        else: st.info("üìä Veuillez d'abord charger les donn√©es Excel.")

    selected_keywords = None
    if st.session_state.gsc_data is not None and cfg['manual_keyword_selection']:
        st.subheader("üéØ S√©lection des Mots-cl√©s √† Analyser")
        available_keywords = sorted(st.session_state.gsc_data['query'].unique().tolist())
        selected_keywords = st.multiselect("S√©lectionnez les mots-cl√©s:", options=available_keywords)
    
    if st.session_state.gsc_data is not None and st.session_state.zip_content is not None:
        can_analyze = not cfg['manual_keyword_selection'] or (cfg['manual_keyword_selection'] and selected_keywords is not None)
        if can_analyze:
            if st.button("üöÄ Lancer l'Analyse Compl√®te", type="primary", use_container_width=True):
                analyzer = InternalLinkingAnalyzer(cfg)
                analyzer.excel_data = st.session_state.gsc_data
                st.session_state.results = analyzer.analyze_opportunities(st.session_state.zip_content, selected_keywords)
        elif cfg['manual_keyword_selection']:
            st.warning("‚ö†Ô∏è Veuillez s√©lectionner au moins un mot-cl√© pour lancer l'analyse.")

    if st.session_state.results is not None:
        if st.session_state.results:
            df_display = pd.DataFrame(st.session_state.results).rename(columns={'source_url': 'URL Source', 'target_url': 'Page √† Mailler', 'anchor': 'Ancre de Lien', 'element_source': '√âl√©ment Source', 'existing_link': 'Lien Existant', 'priority': 'Priorit√©', 'match_type': 'Type de Match', 'anchor_location': 'Source Ancre'})
            st.header("üìã R√©sultats de l'Analyse")
            st.dataframe(df_display[['URL Source', 'Ancre de Lien', 'Source Ancre', 'Page √† Mailler', '√âl√©ment Source', 'Type de Match', 'Lien Existant', 'Priorit√©']], use_container_width=True, column_config={"URL Source": st.column_config.LinkColumn(), "Page √† Mailler": st.column_config.LinkColumn()})
            st.subheader("üì• Export des R√©sultats")
            col_export1, col_export2 = st.columns(2)
            with col_export1:
                st.download_button("üì• T√©l√©charger CSV", df_display.to_csv(index=False, encoding='utf-8-sig'), "opportunites_maillage.csv", "text/csv", use_container_width=True)
            with col_export2:
                if XLSX_EXPORT_AVAILABLE:
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer: df_display.to_excel(writer, index=False, sheet_name='Opportunit√©s')
                    st.download_button("üìÑ T√©l√©charger Excel (.xlsx)", output.getvalue(), "opportunites_maillage.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", use_container_width=True)
                else: st.warning("Pour l'export Excel, installez `openpyxl`", icon="‚ö†Ô∏è")
            st.divider()
            st.header("üìà Tableau de Bord de l'Analyse")
            total_ops, new_ops = len(df_display), len(df_display[df_display['Lien Existant'] == '‚úÖ Nouvelle opportunit√©'])
            col_metric1, col_metric2, col_metric3 = st.columns(3)
            col_metric1.metric("Opportunit√©s Totales", total_ops)
            if total_ops > 0:
                col_metric2.metric("Nouvelles Opportunit√©s ‚úÖ", new_ops, f"{new_ops/total_ops:.1%}")
                col_metric3.metric("Liens D√©j√† Pr√©sents ‚ùå", total_ops - new_ops, f"{(total_ops - new_ops)/total_ops:.1%}")
            col_graph1, col_graph2 = st.columns(2)
            with col_graph1:
                st.write("**Top 10 Pages Sources d'Opportunit√©s**"); st.bar_chart(df_display['URL Source'].value_counts().head(10))
                st.write("**Distribution par Type de Match**"); st.bar_chart(df_display['Type de Match'].value_counts())
            with col_graph2:
                st.write("**Top 10 Pages Cibles (√† mailler)**"); st.bar_chart(df_display['Page √† Mailler'].value_counts().head(10))
                st.write("**Distribution par Source de l'Ancre**"); st.bar_chart(df_display['Source Ancre'].value_counts())
        else:
            st.warning("‚ö†Ô∏è Aucune opportunit√© trouv√©e avec la configuration actuelle.")
            
    st.sidebar.divider()
    if st.sidebar.button("üîÑ Recommencer l'analyse"):
        for key in list(st.session_state.keys()): del st.session_state[key]
        st.cache_data.clear()
        st.rerun()

if __name__ == "__main__":
    if not AHO_CORASICK_AVAILABLE: st.warning("**Performance limit√©e :** `pyahocorasick` non install√© (`pip install pyahocorasick`)", icon="‚ö†Ô∏è")
    if not XLSX_EXPORT_AVAILABLE: st.sidebar.warning("Pour l'export Excel (.xlsx), installez `openpyxl`", icon="‚ö†Ô∏è")
    if not FUZZY_AVAILABLE: st.sidebar.warning("Pour l'analyse floue, installez `fuzzywuzzy` et `python-levenshtein`", icon="‚ö†Ô∏è")
    main()
